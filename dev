Great — below is the **complete project (Option 2)**: full code + documentation (all files).
Copy these files into a folder `test_report_extractor/` (same structure as shown), install the dependencies and run `python main.py`. I included README, config, requirements and all extractor modules.

> Tip: create the folder, then create files with the exact filenames and paste the contents below.

---

# Project structure (what you'll create)

```
test_report_extractor/
├── main.py
├── config.py
├── requirements.txt
├── README.md
└── extractor/
    ├── __init__.py
    ├── file_utils.py
    ├── logger.py
    ├── text_parser.py
    ├── pdf_reader.py
    ├── image_reader.py
    ├── table_extractor.py
    ├── canonicalizer.py
    └── merger.py
```

---

# 1) `README.md`

```markdown
# Test Report Extractor — Full Project (Code + Documentation)

## Overview
This project extracts structured data from PDFs and images (lab reports). It:
- Extracts text (pdfplumber / OCR via Tesseract)
- Extracts tables and flattens them using row labels (first column) + column headers
- Saves images into `output/extracted_images/<HTAC>/...`
- Canonicalizes parameter names (configurable) and merges same parameters across files using fuzzy matching (rapidfuzz)
- Produces consolidated Excel and CSV outputs
- Supports append mode: new files are appended to the existing master Excel
- Marks `extract_status` per file (`OK`, `LOW_CONFIDENCE`, `FAILED`)

## Folder structure
```

test_report_extractor/
├── main.py
├── config.py
├── requirements.txt
├── README.md
└── extractor/
├── **init**.py
├── file_utils.py
├── logger.py
├── text_parser.py
├── pdf_reader.py
├── image_reader.py
├── table_extractor.py
├── canonicalizer.py
└── merger.py

````

## How it works (pipeline)
1. Gather files from `input_reports/`
2. For each file:
   - If PDF: extract text via pdfplumber; if empty, OCR the page images
   - Extract tables using camelot (preferred) then pdfplumber fallback
   - Flatten tables: `table{n}_{RowName}_{ColumnHeader}`
   - Extract images and save under `output/extracted_images/<HTAC>/...`
   - Extract simple `Parameter: Value` pairs from the text
   - Detect HTAC number using regex heuristics
   - Score OCR confidence using `pytesseract.image_to_data`
3. After processing all files:
   - Canonicalize keys using `config.CANONICAL_MAP` + fuzzy merging
   - Merge new extraction rows into existing `output/extracted_report_final.xlsx` (append mode)
   - Save `extracted_report_final.xlsx`, `extracted_report_final.csv`, and `extraction_issues.csv` if any issues

## Installation
1. Install system dependencies:
   - **Tesseract OCR** (system package)
   - **Ghostscript** (for camelot)
2. Create virtualenv and install python requirements:
```bash
python3 -m venv venv
source venv/bin/activate        # or venv\Scripts\activate on Windows
pip install --upgrade pip
pip install -r requirements.txt
````

## Running

Place your PDFs/images in `input_reports/` and run:

```bash
python main.py
```

Outputs are in `output/` (Excel, CSV, extracted_images)

## Append mode

If `output/extracted_report_final.xlsx` exists, new runs add new files (no duplicates by `source_file`).

## Customization

* Edit `config.py` to add canonical mappings and tune thresholds
* For better OCR, pre-process images (deskew, denoise)
* Add new regex patterns for `find_htac` in `text_parser.py` if your HTAC format differs

````

---

# 2) `requirements.txt`
```text
pandas
openpyxl
pdfplumber
pytesseract
Pillow
camelot-py[cv]
pdf2image
rapidfuzz
````

Notes:

* `camelot-py[cv]` usually requires system packages: Ghostscript + Tk (or on Windows the Ghostscript binary).
* `pytesseract` requires Tesseract installed on the OS (not just the Python package).

---

# 3) `config.py`

```python
# config.py
# Global configuration - edit canonical map & thresholds here

# OCR thresholds (tweak for your dataset)
MIN_TEXT_LENGTH = 30            # minimal text length to consider readable
OCR_OK_CONFIDENCE = 40          # avg OCR confidence >= this -> OK
OCR_LOW_CONFIDENCE = 25         # between LOW and OK -> LOW_CONFIDENCE

# Fuzzy threshold for merging parameter/column names (0-100)
FUZZY_THRESHOLD = 85

# Canonical mapping (lowercase normalized keys -> canonical label)
# Add common variants here so they always map to the canonical column name.
CANONICAL_MAP = {
    "carbonblack": "Carbon Black (%)",
    "carbon": "Carbon Black (%)",
    "c": "Carbon Black (%)",
    "wt": "Weight (g)",
    "weight": "Weight (g)",
    "polymer": "Polymer (%)",
    "polymer%": "Polymer (%)",
    "ash": "Ash (%)",
    "ash%": "Ash (%)",
    "hclsolubleash": "HCl Soluble Ash (%)",
    "totalsulfur": "Total Sulfur (%)",
    # add more mappings as needed
}
```

---

# 4) `extractor/__init__.py`

```python
# extractor package init
```

---

# 5) `extractor/logger.py`

```python
# extractor/logger.py
import logging

def get_logger(name="extractor"):
    logging.basicConfig(level=logging.INFO,
                        format="%(asctime)s | %(levelname)s | %(message)s")
    return logging.getLogger(name)
```

---

# 6) `extractor/file_utils.py`

```python
# extractor/file_utils.py
import os
from pathlib import Path

def ensure_dir(p):
    Path(p).mkdir(parents=True, exist_ok=True)

def list_supported_files(input_dir):
    exts = {'.pdf', '.png', '.jpg', '.jpeg', '.tif', '.tiff', '.bmp'}
    files = []
    for root, _, filenames in os.walk(input_dir):
        for fn in filenames:
            if Path(fn).suffix.lower() in exts:
                files.append(os.path.join(root, fn))
    return files
```

---

# 7) `extractor/text_parser.py`

```python
# extractor/text_parser.py
import re
from PIL import ImageOps
import pytesseract

HTAC_REGEXES = [
    r'HTAC\s*No\.?\s*[:\-]?\s*([A-Za-z0-9\-_]+)',
    r'HTAC_No\.?\s*[:\-]?\s*([A-Za-z0-9\-_]+)',
    r'HTAC No[:\s]*([A-Za-z0-9\-_]+)',
    r'TC[-_\s]?\d{3,}'
]

def extract_kv_pairs(text):
    kv = {}
    if not text:
        return kv
    for line in text.splitlines():
        line = line.strip()
        if not line or len(line) > 300:
            continue
        m = re.match(r'([\w\-\s\(\)%/\.]+?)\s*[:\-]\s*(.+)', line)
        if m:
            k = ' '.join(m.group(1).split())
            v = m.group(2).strip()
            kv[k] = v
    return kv

def find_htac(text):
    if not text:
        return None
    for pattern in HTAC_REGEXES:
        m = re.search(pattern, text, re.IGNORECASE)
        if m:
            return m.group(1).strip()
    return None

def find_test_name(text):
    t = (text or '').lower()
    if 'physical' in t: return 'Physical Test'
    if 'chemical' in t: return 'Chemical Test'
    if 'analytical' in t or 'analysis' in t: return 'Analytical Test'
    return 'General Test'

def ocr_image_get_text_and_conf(pil_img):
    """
    Returns (text_string, average_confidence) for a PIL image using pytesseract.
    """
    gray = ImageOps.grayscale(pil_img)
    data = pytesseract.image_to_data(gray, output_type=pytesseract.Output.DICT, lang='eng')
    texts = []
    confs = []
    n = len(data['text'])
    for i in range(n):
        txt = (data['text'][i] or '').strip()
        try:
            conf = float(data['conf'][i])
        except Exception:
            conf = -1.0
        if txt:
            texts.append(txt)
            if conf >= 0:
                confs.append(conf)
    avg_conf = sum(confs)/len(confs) if confs else -1.0
    return ' '.join(texts), avg_conf
```

---

# 8) `extractor/pdf_reader.py`

```python
# extractor/pdf_reader.py
import pdfplumber
from extractor.text_parser import ocr_image_get_text_and_conf
from extractor.file_utils import ensure_dir
import os

def extract_text_and_conf_from_pdf(path, ocr_resolution=200):
    """
    Try pdfplumber text extraction first; if empty use OCR per page.
    Returns: (text_combined, avg_confidence)
    """
    text_all = []
    confs = []
    try:
        with pdfplumber.open(path) as pdf:
            for page in pdf.pages:
                txt = page.extract_text() or ''
                if txt and len(txt.strip()) > 10:
                    text_all.append(txt)
                    confs.append(95.0)  # assume high for digital text
                else:
                    # do OCR of the page image
                    img = page.to_image(resolution=ocr_resolution).original
                    t, c = ocr_image_get_text_and_conf(img)
                    text_all.append(t)
                    if c >= 0:
                        confs.append(c)
    except Exception:
        # fallback: try OCR of entire PDF by rendering pages if pdfplumber fails
        try:
            with pdfplumber.open(path) as pdf:
                for page in pdf.pages:
                    img = page.to_image(resolution=ocr_resolution).original
                    t, c = ocr_image_get_text_and_conf(img)
                    text_all.append(t)
                    if c >= 0:
                        confs.append(c)
        except Exception:
            return '', -1.0
    combined = '\n'.join(text_all).strip()
    avg_conf = (sum([c for c in confs if c>0]) / len([c for c in confs if c>0])) if any(c>0 for c in confs) else -1.0
    return combined, avg_conf

def extract_images_from_pdf(path, out_dir, htac):
    saved = []
    ensure_dir(out_dir)
    try:
        with pdfplumber.open(path) as pdf:
            for i, page in enumerate(pdf.pages, start=1):
                try:
                    img = page.to_image(resolution=150).original
                    fname = f"{htac}_page{i}.png"
                    outp = os.path.join(out_dir, fname)
                    img.save(outp)
                    saved.append(outp)
                    # try cropping embedded images (if any)
                    for idx, im in enumerate(page.images or [], start=1):
                        try:
                            pil = img.original.crop((im['x0'], im['top'], im['x1'], im['bottom']))
                            fname2 = f"{htac}_p{i}_img{idx}.png"
                            outp2 = os.path.join(out_dir, fname2)
                            pil.save(outp2)
                            saved.append(outp2)
                        except Exception:
                            continue
                except Exception:
                    continue
    except Exception:
        pass
    return saved
```

---

# 9) `extractor/image_reader.py`

```python
# extractor/image_reader.py
from PIL import Image
from extractor.text_parser import ocr_image_get_text_and_conf
import shutil
import os

def process_image_file(path, out_images_root, htac):
    try:
        pil = Image.open(path)
    except Exception:
        pil = None
    text, conf = ('', -1.0)
    saved = []
    if pil:
        text, conf = ocr_image_get_text_and_conf(pil)
    # save the original image into the htac folder
    htac_folder = os.path.join(out_images_root, htac)
    os.makedirs(htac_folder, exist_ok=True)
    dest = os.path.join(htac_folder, os.path.basename(path))
    try:
        shutil.copy(path, dest)
        saved.append(dest)
    except Exception:
        pass
    return text, conf, saved
```

---

# 10) `extractor/table_extractor.py`

```python
# extractor/table_extractor.py
import camelot
import pdfplumber
import pandas as pd
import re

def clean_name(name):
    if name is None:
        return "Unknown"
    s = str(name).strip()
    s = re.sub(r'[^A-Za-z0-9]+', '', s)
    if s == "":
        return "Unknown"
    return s

def extract_tables_from_pdf(path):
    tables = []
    # try camelot (works well for digital PDFs / good table borders)
    try:
        camelot_tables = camelot.read_pdf(path, pages='all', flavor='stream')
        for t in camelot_tables:
            df = t.df
            # Sometimes first row is header; if header looks numeric we still keep as strings
            tables.append(df)
    except Exception:
        pass

    # fallback to pdfplumber table extraction
    try:
        with pdfplumber.open(path) as pdf:
            for page in pdf.pages:
                try:
                    for tbl in page.extract_tables():
                        df = pd.DataFrame(tbl[1:], columns=tbl[0])
                        tables.append(df)
                except Exception:
                    continue
    except Exception:
        pass

    return tables

def flatten_table_with_row_labels(df, table_index):
    """
    Expects first column to contain the row label names.
    Produces dict with keys: table{index}_{RowName}_{ColumnHeader}
    """
    flat = {}
    # Ensure DataFrame and columns are strings
    df = pd.DataFrame(df)
    df.columns = [str(c) for c in df.columns]
    headers = [clean_name(h) for h in df.columns]

    # if there is only one column (no values) skip
    if len(headers) < 2:
        return flat

    for r in range(len(df)):
        raw_row = df.iat[r, 0]
        row_name = clean_name(raw_row)
        for c in range(1, len(headers)):
            col_header = clean_name(df.columns[c])
            val = df.iat[r, c]
            key = f"table{table_index}_{row_name}_{col_header}"
            flat[key] = val
    return flat
```

---

# 11) `extractor/canonicalizer.py`

```python
# extractor/canonicalizer.py
import re
from rapidfuzz import process, fuzz
from config import CANONICAL_MAP, FUZZY_THRESHOLD

def normalize_key(k):
    if k is None:
        return "unknown"
    s = str(k).lower()
    s = re.sub(r'[^a-z0-9]+', '', s)
    return s

def canonicalize(key):
    """
    Return canonical name for a parameter key.
    Uses direct mapping first, then fuzzy match against mapped keys, otherwise
    returns cleaned title-like name.
    """
    if key is None:
        return "Unknown"

    norm = normalize_key(key)

    # direct map
    if norm in CANONICAL_MAP:
        return CANONICAL_MAP[norm]

    # fuzzy match against canonical map keys
    if CANONICAL_MAP:
        best = process.extractOne(norm, list(CANONICAL_MAP.keys()), scorer=fuzz.token_sort_ratio)
        if best and best[1] >= FUZZY_THRESHOLD:
            return CANONICAL_MAP[best[0]]

    # else return cleaned title-case which is Excel-friendly
    s = re.sub(r'[^A-Za-z0-9]+', ' ', str(key)).strip()
    return ''.join([w.capitalize() for w in s.split()])
```

---

# 12) `extractor/merger.py`

```python
# extractor/merger.py
import pandas as pd
from extractor.canonicalizer import canonicalize

def merge_records(records):
    """
    records: list of dicts with keys:
      source, htac, test, text, confidence, status, images (semicolon-separated),
      kv (dict), table_cells (list of dicts)
    Produces consolidated DataFrame where columns are canonicalized across all records.
    """
    # gather all keys
    all_keys = set()
    for r in records:
        for k in (r.get('kv') or {}).keys():
            all_keys.add(k)
        for tc in (r.get('table_cells') or []):
            for k in tc.keys():
                all_keys.add(k)

    # build map original_key -> canonical_key
    mapping = {k: canonicalize(k) for k in all_keys}

    rows = []
    for r in records:
        row = {
            'source_file': r.get('source'),
            'htac': r.get('htac'),
            'test_name': r.get('test'),
            'image_paths': r.get('images'),
            'raw_text': (r.get('text') or '')[:5000],
            'ocr_confidence': r.get('confidence'),
            'extract_status': r.get('status')
        }
        # add kv pairs
        for k, v in (r.get('kv') or {}).items():
            row[mapping.get(k, k)] = v
        # add table cells
        for tc in (r.get('table_cells') or []):
            for k, v in tc.items():
                row[mapping.get(k, k)] = v
        rows.append(row)

    df = pd.DataFrame(rows)
    # order fixed columns first
    fixed = ['source_file', 'htac', 'test_name', 'image_paths', 'raw_text', 'ocr_confidence', 'extract_status']
    others = [c for c in df.columns if c not in fixed]
    df = df[fixed + sorted(others)]
    return df
```

---

# 13) `main.py`

```python
# main.py
import os
from pathlib import Path
from extractor.logger import get_logger
from extractor.file_utils import ensure_dir, list_supported_files
from extractor.pdf_reader import extract_text_and_conf_from_pdf, extract_images_from_pdf
from extractor.image_reader import process_image_file
from extractor.table_extractor import extract_tables_from_pdf, flatten_table_with_row_labels
from extractor.text_parser import extract_kv_pairs, find_htac, find_test_name
from extractor.merger import merge_records
import pandas as pd
from config import MIN_TEXT_LENGTH, OCR_OK_CONFIDENCE, OCR_LOW_CONFIDENCE

log = get_logger("main")

def load_existing_output(output_dir):
    excel_path = Path(output_dir) / "extracted_report_final.xlsx"
    if excel_path.exists():
        try:
            df = pd.read_excel(excel_path)
            print("Loaded existing Excel:", excel_path)
            return df
        except Exception as e:
            print("Error loading old Excel:", e)
    return pd.DataFrame()

def process_file(path, out_images_root):
    ext = Path(path).suffix.lower()
    # default record
    rec = {
        'source': path,
        'htac': Path(path).stem,
        'test': 'General Test',
        'text': '',
        'confidence': -1.0,
        'status': 'FAILED',
        'images': '',
        'kv': {},
        'table_cells': []
    }

    if ext == '.pdf':
        text, conf = extract_text_and_conf_from_pdf(path)
        # determine HTAC
        htac = find_htac(text) or Path(path).stem
        rec['text'] = text
        rec['confidence'] = conf
        rec['htac'] = htac
        rec['test'] = find_test_name(text)
        # extract images into HTAC folder
        htac_folder = os.path.join(out_images_root, htac)
        images = extract_images_from_pdf(path, htac_folder, htac)
        rec['images'] = ';'.join(images)
        # kv pairs
        rec['kv'] = extract_kv_pairs(text)
        # tables
        tables = extract_tables_from_pdf(path)
        table_cells = []
        for i, t in enumerate(tables, start=1):
            try:
                flat = flatten_table_with_row_labels(t, i)
                if flat:
                    table_cells.append(flat)
            except Exception:
                continue
        rec['table_cells'] = table_cells
    else:
        # image file
        text, conf, images = process_image_file(path, out_images_root, Path(path).stem)
        htac = find_htac(text) or Path(path).stem
        rec['text'] = text
        rec['confidence'] = conf
        rec['htac'] = htac
        rec['test'] = find_test_name(text)
        rec['images'] = ';'.join(images)
        rec['kv'] = extract_kv_pairs(text)
        rec['table_cells'] = []

    # status heuristics
    text_len = len((rec.get('text') or '').strip())
    conf_val = rec.get('confidence') or -1.0
    if text_len >= MIN_TEXT_LENGTH and conf_val >= OCR_OK_CONFIDENCE:
        rec['status'] = 'OK'
    elif text_len >= MIN_TEXT_LENGTH and conf_val >= OCR_LOW_CONFIDENCE:
        rec['status'] = 'LOW_CONFIDENCE'
    else:
        rec['status'] = 'FAILED'

    return rec

def run(input_dir='input_reports', output_dir='output'):
    input_dir = Path(input_dir)
    output_dir = Path(output_dir)
    ensure_dir(output_dir)
    out_images_root = output_dir / 'extracted_images'
    ensure_dir(out_images_root)

    files = list_supported_files(str(input_dir))
    log.info(f"Found {len(files)} files in {input_dir}")

    records = []
    for f in files:
        log.info(f"Processing: {f}")
        try:
            r = process_file(f, str(out_images_root))
            records.append(r)
        except Exception as e:
            log.exception("Failed processing file %s: %s", f, e)
            records.append({
                'source': f,
                'htac': Path(f).stem,
                'test': 'General Test',
                'text': '',
                'confidence': -1.0,
                'status': 'FAILED',
                'images': '',
                'kv': {},
                'table_cells': []
            })

    if not records:
        log.error("No records processed.")
        return

    new_df = merge_records(records)

    # Load old Excel (if exists)
    old_df = load_existing_output(output_dir)

    if not old_df.empty:
        combined_df = pd.concat([old_df, new_df], ignore_index=True)
        combined_df.drop_duplicates(subset=['source_file'], keep='last', inplace=True)
    else:
        combined_df = new_df

    out_xlsx = output_dir / 'extracted_report_final.xlsx'
    combined_df.to_excel(out_xlsx, index=False)
    combined_df.to_csv(output_dir / "extracted_report_final.csv", index=False)

    # save issues
    issues = [ {'source_file': r['source'], 'htac': r['htac'], 'status': r['status'], 'ocr_confidence': r['confidence']} for r in records if r['status'] != 'OK' ]
    if issues:
        pd.DataFrame(issues).to_csv(output_dir / 'extraction_issues.csv', index=False)

    log.info("Saved outputs to %s", output_dir)
    log.info("Done.")

if __name__ == "__main__":
    run()
```

---

# How to install and run — step-by-step (copy-paste)

**1. System dependencies**

Ubuntu example:

```bash
sudo apt update
sudo apt install -y python3 python3-venv python3-pip build-essential
sudo apt install -y tesseract-ocr libtesseract-dev
sudo apt install -y ghostscript
sudo apt install -y tk
```

macOS (Homebrew):

```bash
brew install tesseract
brew install ghostscript
```

Windows:

* Install Tesseract: [https://github.com/tesseract-ocr/tesseract](https://github.com/tesseract-ocr/tesseract) (installer). Note path e.g. `C:\Program Files\Tesseract-OCR\` and add to PATH.
* Install Ghostscript: [https://www.ghostscript.com/](https://www.ghostscript.com/)

**2. Create project folder and virtualenv**

```bash
mkdir test_report_extractor
cd test_report_extractor
# create files by pasting the above content into respective files
python3 -m venv venv
source venv/bin/activate   # or venv\Scripts\activate on Windows
pip install --upgrade pip
pip install -r requirements.txt
```

**3. Run**

* Place your PDF/image files in `input_reports/` (create if absent).
* Run:

```bash
python main.py
```

* Outputs appear in `output/`:

  * `extracted_report_final.xlsx`
  * `extracted_report_final.csv`
  * `extraction_issues.csv` (if any)
  * `extracted_images/<HTAC>/...`

---

# Notes, tips & troubleshooting

* Tesseract not found: ensure tesseract is installed and on PATH (`tesseract --version`).
* Camelot errors: ensure Ghostscript installed. If camelot fails, pdfplumber fallback still extracts some tables.
* If a column appears multiple times, edit `config.CANONICAL_MAP` to merge variants.
* If Excel becomes wide, open in Excel/LibreOffice which handle many columns.
* For scanned noisy pages, consider preprocessing (OpenCV) to deskew or threshold.

---

# Want me to do one of these next?

* I can generate a single downloadable PDF containing the entire project (I attempted but the environment had an error — I can try again and deliver the PDF if you want).
* I can zip the project and provide a download link.
* I can open and process a couple of your sample images now to show the extractor working and provide a sample `extracted_report_final.xlsx`. (If you want this, I will run the extractor on the images you uploaded.)

Tell me which next step you want: `pdf`, `zip`, or `demo` (I will run on your sample images).
