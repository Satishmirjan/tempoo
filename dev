Nice — you chose Option C (testCategory + tableName + rowName + columnName).
Below is the final, complete, production-ready project (every file) updated exactly to:
	•	produce column names in the format:
testname_tablename_rowname_columnname
(all normalized / cleaned / safe for Excel)
	•	sanitize every value (removes illegal control characters)
	•	place NULL for empty cells
	•	append new parameters at the end on repeated runs
	•	preserve one row per source file
	•	extract PDF or image content, tables, and images

Copy all files exactly into your project folder and run python main.py.

Tip: I saw your uploaded image at /mnt/data/IMG_FA784279-AEC8-4419-A288-4102E5976563.jpeg.
If you want to test the extractor immediately using that exact image, copy it into input_reports/ (or leave it where it is and put its path into the input_reports folder or adjust main.py to process that path).

⸻

Directory layout

test_report_extractor/
├── main.py
├── config.py
├── requirements.txt
├── README.md
├── extractor/
│   ├── _init_.py
│   ├── logger.py
│   ├── file_utils.py
│   ├── sanitizer.py
│   ├── text_parser.py
│   ├── pdf_reader.py
│   ├── image_reader.py
│   ├── table_extractor.py
│   ├── canonicalizer.py
│   └── merger.py
└── input_reports/    # put your PDFs/images here


⸻

requirements.txt

pandas
openpyxl
pdfplumber
pytesseract
Pillow
camelot-py[cv]
pdf2image
rapidfuzz

(Install system deps: tesseract, ghostscript, tk for camelot.)

⸻

config.py

# config.py
MIN_TEXT_LENGTH = 30
OCR_OK_CONFIDENCE = 40
OCR_LOW_CONFIDENCE = 25

FUZZY_THRESHOLD = 85

# map normalized keys to canonical short names (used by canonicalizer)
CANONICAL_MAP = {
    "carbonblack": "carbonblack",
    "polymer": "polymer",
    "ash": "ash",
    "weight": "weight",
    "tensilestrength": "tensilestrength",
    # add more as needed
}

TEST_NAME_MAP = {
    "physical": "physical",
    "chemical": "chemical",
    "analytical": "analytical",
    "dimension": "dimension",
    "valve": "valve",
    "general": "general",
}


⸻

extractor/init.py

# extractor package


⸻

extractor/logger.py

import logging

def get_logger(name="extractor"):
    logging.basicConfig(level=logging.INFO,
                        format="%(asctime)s | %(levelname)s | %(message)s")
    return logging.getLogger(name)


⸻

extractor/file_utils.py

import os
from pathlib import Path

def ensure_dir(p):
    Path(p).mkdir(parents=True, exist_ok=True)

def list_supported_files(input_dir):
    exts = {'.pdf', '.png', '.jpg', '.jpeg', '.tif', '.tiff', '.bmp'}
    files = []
    for root, _, filenames in os.walk(input_dir):
        for fn in filenames:
            if Path(fn).suffix.lower() in exts:
                files.append(os.path.join(root, fn))
    return files


⸻

extractor/sanitizer.py

# extractor/sanitizer.py
import re

def clean_excel_value(val):
    """
    Clean strings so they are safe to write into Excel (openpyxl).
    Removes ASCII control characters and trims whitespace.
    Returns string or numeric unchanged except for cleaning.
    """
    if val is None:
        return ""

    # keep numeric types as-is
    try:
        # if it's a number, just return as-is (but convert to python native types)
        if isinstance(val, (int, float, complex)) and not isinstance(val, bool):
            return val
    except Exception:
        pass

    s = str(val)

    # Remove control characters (ASCII 0-31 except \t, \n, \r)
    illegal = ''.join(chr(c) for c in range(32) if c not in (9,10,13))
    s = s.translate(str.maketrans('', '', illegal))

    # Remove other non-printable
    s = re.sub(r'[\x00-\x1F\x7F]', '', s)

    # Replace multiple spaces/newlines with single space (but keep newlines trimmed)
    s = s.strip()

    return s


⸻

extractor/text_parser.py

# extractor/text_parser.py
import re
from PIL import ImageOps
import pytesseract

HTAC_REGEXES = [
    r'HTAC\s*No\.?\s*[:\-]?\s*([A-Za-z0-9\-_]+)',
    r'HTAC_No\.?\s*[:\-]?\s*([A-Za-z0-9\-_]+)',
    r'HTAC No[:\s]*([A-Za-z0-9\-_]+)',
]

def extract_kv_pairs(text):
    kv = {}
    if not text:
        return kv
    for line in text.splitlines():
        line = line.strip()
        if not line or len(line) > 400:
            continue
        m = re.match(r'(.+?)\s*[:\-]\s*(.+)', line)
        if m:
            k = m.group(1).strip()
            v = m.group(2).strip()
            kv[k] = v
    return kv

def find_htac(text):
    if not text:
        return None
    for pattern in HTAC_REGEXES:
        m = re.search(pattern, text, re.IGNORECASE)
        if m:
            return m.group(1).strip()
    return None

def find_test_name(text):
    t = (text or "").lower()
    if "physical" in t: return "physical"
    if "chemical" in t or "composition" in t: return "chemical"
    if "analytical" in t or "analysis" in t: return "analytical"
    if "dimension" in t: return "dimension"
    if "valve" in t: return "valve"
    return "general"

def ocr_image_get_text_and_conf(pil_img):
    """
    Returns (text_string, avg_confidence) for PIL image using pytesseract.
    """
    gray = ImageOps.grayscale(pil_img)
    data = pytesseract.image_to_data(gray, output_type=pytesseract.Output.DICT, lang='eng')
    texts = []
    confs = []
    for i, t in enumerate(data.get('text', [])):
        txt = (t or '').strip()
        if not txt:
            continue
        texts.append(txt)
        try:
            c = float(data['conf'][i])
            if c >= 0:
                confs.append(c)
        except Exception:
            pass
    avg_conf = sum(confs)/len(confs) if confs else -1.0
    return ' '.join(texts), avg_conf


⸻

extractor/pdf_reader.py

# extractor/pdf_reader.py
import pdfplumber
from extractor.text_parser import ocr_image_get_text_and_conf
from extractor.file_utils import ensure_dir
import os

def extract_text_and_conf_from_pdf(path, ocr_resolution=200):
    texts = []
    confs = []
    try:
        with pdfplumber.open(path) as pdf:
            for page in pdf.pages:
                txt = page.extract_text() or ""
                if txt and len(txt.strip()) > 8:
                    texts.append(txt)
                    confs.append(95.0)
                else:
                    img = page.to_image(resolution=ocr_resolution).original
                    t, c = ocr_image_get_text_and_conf(img)
                    texts.append(t)
                    if c >= 0:
                        confs.append(c)
    except Exception:
        return "", -1.0
    combined = '\n'.join(texts).strip()
    avg_conf = sum(confs)/len(confs) if confs else -1.0
    return combined, avg_conf

def extract_images_from_pdf(path, out_dir, htac):
    saved = []
    ensure_dir(out_dir)
    try:
        with pdfplumber.open(path) as pdf:
            for i, page in enumerate(pdf.pages, start=1):
                try:
                    img = page.to_image(resolution=150).original
                    fname = f"{htac}_page{i}.png"
                    outp = os.path.join(out_dir, fname)
                    img.save(outp)
                    saved.append(outp)
                except Exception:
                    continue
    except Exception:
        pass
    return saved


⸻

extractor/image_reader.py

# extractor/image_reader.py
from PIL import Image
from extractor.text_parser import ocr_image_get_text_and_conf
import shutil, os
from extractor.file_utils import ensure_dir

def process_image_file(path, out_images_root, htac):
    try:
        pil = Image.open(path)
    except Exception:
        return "", -1.0, []
    text, conf = ocr_image_get_text_and_conf(pil)
    htac_dir = os.path.join(out_images_root, htac)
    ensure_dir(htac_dir)
    dest = os.path.join(htac_dir, os.path.basename(path))
    try:
        shutil.copy(path, dest)
    except Exception:
        pass
    return text, conf, [dest]


⸻

extractor/canonicalizer.py

# extractor/canonicalizer.py
import re
from rapidfuzz import process, fuzz
from config import CANONICAL_MAP, FUZZY_THRESHOLD, TEST_NAME_MAP

def normalize(k):
    return re.sub(r'[^a-z0-9]+', '', str(k or "").lower())

def canonicalize_key(key):
    """
    Return canonical short key for column/row names (like 'carbonblack', 'tensilestrength').
    """
    if key is None:
        return "unknown"
    norm = normalize(key)
    if norm in CANONICAL_MAP:
        return CANONICAL_MAP[norm]
    if CANONICAL_MAP:
        best = process.extractOne(norm, list(CANONICAL_MAP.keys()), scorer=fuzz.token_sort_ratio)
        if best and best[1] >= FUZZY_THRESHOLD:
            return CANONICAL_MAP[best[0]]
    # fallback: compact normalized lowercase token
    return norm if norm else "unknown"

def canonicalize_testname(name):
    n = normalize(name)
    return TEST_NAME_MAP.get(n, name if name else "general")


⸻

extractor/table_extractor.py

# extractor/table_extractor.py
import camelot
import pdfplumber
import pandas as pd
import re
from extractor.canonicalizer import canonicalize_key, canonicalize_testname
from extractor.sanitizer import clean_excel_value

def clean_simple(s):
    s = str(s or "")
    s = s.strip()
    # remove repeated whitespace/newlines
    s = re.sub(r'\s+', ' ', s)
    return s

def extract_tables_from_pdf(path):
    tables = []
    try:
        c = camelot.read_pdf(path, pages='all', flavor='stream')
        for t in c:
            df = t.df
            tables.append(df)
    except Exception:
        pass

    try:
        with pdfplumber.open(path) as pdf:
            for p in pdf.pages:
                tbs = p.extract_tables()
                for tb in (tbs or []):
                    if not tb: 
                        continue
                    df = pd.DataFrame(tb[1:], columns=tb[0])
                    tables.append(df)
    except Exception:
        pass

    return tables

def make_safe_part(s):
    # canonical short key for part used inside final column name
    return canonicalize_key(clean_simple(s))

def make_safe_table_name(s):
    # Use header/title-like string or a short label; fallback to 'table'
    return make_safe_part(s) or "table"

def flatten_table_with_row_labels(df, table_index, test_name, table_name_hint=None):
    """
    Produce dict where each key is:
      testname_tablename_rowname_columnname
    Values are cleaned values (strings) or 'NULL'
    """
    flat = {}
    if df is None or len(df.columns) < 2:
        return flat

    # standardize dataframe
    df = df.copy()
    df = df.fillna("NULL")
    # guess table name from hint or first header
    if table_name_hint:
        tname = make_safe_table_name(table_name_hint)
    else:
        # use first non-empty header or 'table'
        first_header = df.columns[0] if df.columns.size else f"table{table_index}"
        tname = make_safe_table_name(first_header)

    test_cat = canonicalize_testname(test_name).lower()
    # normalize test_cat to safe token
    test_cat = re.sub(r'[^a-z0-9]+', '', str(test_cat).lower()) or "general"

    # if first row is header row (duplicate header names), we still treat df as given.
    # assume first column contains row names
    for r in range(len(df)):
        raw_row = df.iat[r, 0]
        row_key = make_safe_part(raw_row) or f"row{r+1}"

        for c in range(1, len(df.columns)):
            raw_col = df.columns[c]
            col_key = make_safe_part(raw_col) or f"col{c+1}"

            raw_val = df.iat[r, c]
            val = raw_val if raw_val not in [None, "", "NULL"] else "NULL"
            # sanitize
            val = clean_excel_value(val)

            # final key: test_tablename_row_col
            final_key = f"{test_cat}{tname}{row_key}_{col_key}"
            # normalize underscores: collapse multiple underscores
            final_key = re.sub(r'+', '', final_key).strip('').lower()

            flat[final_key] = val if val != "" else "NULL"

    return flat


⸻

extractor/merger.py

# extractor/merger.py
import pandas as pd
from extractor.sanitizer import clean_excel_value

def merge_records(records):
    """
    Merge extracted records (list of dicts) into a DataFrame.
    Each record has: source, htac, test, images, text, confidence, status, kv(dict), table_cells(list of dict)
    """
    all_keys = set()

    for r in records:
        # kv pairs
        for k in (r.get('kv') or {}).keys():
            all_keys.add(k)
        # flattened table keys
        for tb in (r.get('table_cells') or []):
            for k in tb.keys():
                all_keys.add(k)

    # We'll not transform flattened table keys here; table_extractor already created safe keys.
    # For kv keys (textual key-value metadata), sanitize keys to safe column names
    def safe_col_name(k):
        # remove non-alnum chars and lowercase, replace spaces with underscore
        s = str(k or "")
        s = s.strip()
        s = s.replace(' ', '_')
        s = ''.join(ch for ch in s if (ch.isalnum() or ch == '_'))
        s = s.lower()
        return s if s else "kv"

    # Build rows
    rows = []
    for r in records:
        row = {
            'source_file': clean_excel_value(r.get('source')),
            'htac': clean_excel_value(r.get('htac')),
            'test_name': clean_excel_value(r.get('test')),
            'image_paths': clean_excel_value(r.get('images')),
            'raw_text': clean_excel_value((r.get('text') or '')[:5000]),
            'ocr_confidence': r.get('confidence'),
            'extract_status': clean_excel_value(r.get('status'))
        }
        # KV pairs
        for k, v in (r.get('kv') or {}).items():
            row[safe_col_name(k)] = clean_excel_value(v)
        # Table flattened cells (already safe keys)
        for tb in (r.get('table_cells') or []):
            for k, v in tb.items():
                # k assumed safe like test_table_row_col
                row[k] = clean_excel_value(v)

        rows.append(row)

    df = pd.DataFrame(rows)

    # reorder columns to keep fixed columns first
    fixed = ['source_file', 'htac', 'test_name', 'image_paths', 'raw_text', 'ocr_confidence', 'extract_status']
    others = [c for c in df.columns if c not in fixed]
    df = df[fixed + sorted(others)]
    return df


⸻

main.py

# main.py
import os
from pathlib import Path
import pandas as pd
from extractor.logger import get_logger
from extractor.file_utils import ensure_dir, list_supported_files
from extractor.pdf_reader import extract_text_and_conf_from_pdf, extract_images_from_pdf
from extractor.image_reader import process_image_file
from extractor.table_extractor import extract_tables_from_pdf, flatten_table_with_row_labels
from extractor.text_parser import extract_kv_pairs, find_htac, find_test_name
from extractor.merger import merge_records
from extractor.sanitizer import clean_excel_value
from config import MIN_TEXT_LENGTH, OCR_OK_CONFIDENCE, OCR_LOW_CONFIDENCE

log = get_logger("main")

def load_existing_excel(output_dir):
    path = Path(output_dir) / "extracted_report_final.xlsx"
    if path.exists():
        try:
            return pd.read_excel(path)
        except Exception as e:
            log.exception("Error loading existing Excel: %s", e)
    return pd.DataFrame()

def process_file(path, out_images_root):
    ext = Path(path).suffix.lower()
    rec = {
        "source": path,
        "htac": Path(path).stem,
        "test": "general",
        "text": "",
        "confidence": -1,
        "status": "FAILED",
        "images": "",
        "kv": {},
        "table_cells": []
    }

    if ext == ".pdf":
        text, conf = extract_text_and_conf_from_pdf(path)
        rec["text"] = text
        rec["confidence"] = conf
        rec["htac"] = find_htac(text) or rec["htac"]
        rec["test"] = find_test_name(text)

        # images
        out_img_dir = os.path.join(out_images_root, rec["htac"])
        imgs = extract_images_from_pdf(path, out_img_dir, rec["htac"])
        rec["images"] = ';'.join(imgs)

        rec["kv"] = extract_kv_pairs(text)

        tables = extract_tables_from_pdf(path)
        cells = []
        for idx, t in enumerate(tables, start=1):
            try:
                # optional: pass a table name hint; here keep None
                flat = flatten_table_with_row_labels(t, idx, rec["test"], table_name_hint=None)
                if flat:
                    cells.append(flat)
            except Exception:
                continue
        rec["table_cells"] = cells

    else:
        text, conf, imgs = process_image_file(path, out_images_root, rec["htac"])
        rec["text"] = text
        rec["confidence"] = conf
        rec["images"] = ';'.join(imgs)
        rec["kv"] = extract_kv_pairs(text)
        rec["test"] = find_test_name(text)
        rec["table_cells"] = []

    # determine status
    if len((rec.get("text") or "").strip()) >= MIN_TEXT_LENGTH and rec.get("confidence", -1) >= OCR_OK_CONFIDENCE:
        rec["status"] = "OK"
    elif len((rec.get("text") or "").strip()) >= MIN_TEXT_LENGTH and rec.get("confidence", -1) >= OCR_LOW_CONFIDENCE:
        rec["status"] = "LOW_CONFIDENCE"
    else:
        rec["status"] = "FAILED"

    return rec

def run(input_dir="input_reports", output_dir="output"):
    input_dir = Path(input_dir)
    output_dir = Path(output_dir)
    ensure_dir(output_dir)
    ensure_dir(output_dir / "extracted_images")

    files = list_supported_files(str(input_dir))
    log.info(f"Found {len(files)} files to process in {input_dir}")

    existing = load_existing_excel(output_dir)

    records = []
    for f in files:
        try:
            log.info(f"Processing file: {f}")
            rec = process_file(f, str(output_dir / "extracted_images"))
            records.append(rec)
        except Exception as e:
            log.exception("Failed processing %s: %s", f, e)
            # append minimal failed record
            records.append({
                "source": f,
                "htac": Path(f).stem,
                "test": "general",
                "text": "",
                "confidence": -1,
                "status": "FAILED",
                "images": "",
                "kv": {},
                "table_cells": []
            })

    if not records:
        log.warning("No records processed. Exiting.")
        return

    df_new = merge_records(records)

    # append mode: combine existing + new and dedupe by source_file
    if not existing.empty:
        combined = pd.concat([existing, df_new], ignore_index=True)
        if 'source_file' in combined.columns:
            combined.drop_duplicates(subset=['source_file'], keep='last', inplace=True)
        df_out = combined
    else:
        df_out = df_new

    # sanitize all values as final pass
    df_out = df_out.applymap(clean_excel_value)

    # write outputs
    out_xlsx = output_dir / "extracted_report_final.xlsx"
    out_csv = output_dir / "extracted_report_final.csv"
    df_out.to_excel(out_xlsx, index=False)
    df_out.to_csv(out_csv, index=False)

    # save issues (non-OK)
    issues = [ {'source_file': r['source'], 'htac': r['htac'], 'status': r['status'], 'ocr_confidence': r['confidence']} for r in records if r['status'] != 'OK' ]
    if issues:
        import pandas as pd
        pd.DataFrame(issues).to_csv(output_dir / 'extraction_issues.csv', index=False)

    log.info("Saved outputs to %s", output_dir)
    log.info("Done.")

if _name_ == "_main_":
    run()


⸻

README.md (brief)

# Test Report Extractor — Final (test_tab_row_col format)

## Setup
1. Install system packages:
   - Linux/Ubuntu: tesseract-ocr, ghostscript, tk
   - macOS: brew install tesseract ghostscript
   - Windows: install Tesseract & Ghostscript; add to PATH.

2. Python deps:
   python -m venv venv
   source venv/bin/activate    # or venv\Scripts\activate on Windows
   pip install -r requirements.txt

3. Put PDFs/images into input_reports/.
   - If you already have /mnt/data/IMG_FA784279-AEC8-4419-A288-4102E5976563.jpeg,
     copy it to input_reports/ or move it there:
     cp /mnt/data/IMG_FA784279-AEC8-4419-A288-4102E5976563.jpeg input_reports/

4. Run:
   python main.py

5. Outputs:
   - output/extracted_report_final.xlsx
   - output/extracted_report_final.csv
   - output/extraction_issues.csv (if any)
   - output/extracted_images/<HTAC>/...


⸻

Quick notes & troubleshooting
	•	Column naming: Each extracted table cell becomes a column named:
testname_tablename_rowname_columnname (lowercase, underscores, sanitized).
	•	NULL values: Empty table cells become "NULL".
	•	Sanitization: All values pass through clean_excel_value to remove illegal characters (fixes the IllegalCharacterError you saw).
	•	If camelot fails: pdfplumber fallback is used for tables.
	•	Append behavior: Running again will add new files and dedupe by source_file.
	•	Custom canonical mapping: Edit CANONICAL_MAP in config.py to canonicalize frequent field names.
	•	Test with the uploaded file: the file you uploaded earlier is at /mnt/data/IMG_FA784279-AEC8-4419-A288-4102E5976563.jpeg. Copy it to input_reports/ to test.

⸻

If you want, I can do one of the following right now:
	•	A. Package this full project into a downloadable ZIP and give you a link (I’ll create it here and return the link). — Reply ZIP
	•	B. Run the extractor on the file you already uploaded (/mnt/data/IMG_FA784279-AEC8-4419-A288-4102E5976563.jpeg) and return the resulting extracted_report_final.xlsx and logs so you can inspect output. — Reply RUN_ON_UPLOADED
	•	C. I’ll instead run the extractor on any other local file path you give me (paste the path). — Reply RUN_ON_PATH <path>

Which one would you like next?
